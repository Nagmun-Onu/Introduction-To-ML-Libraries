{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeDPHq6k3k5X"
   },
   "source": [
    "# COMP1816 Tutorial Week 5 - Numerical Optimisation and feature scaling\n",
    "\n",
    "This tutorial will be focussing on how we can use numerical optimisation to obtain a minimal cost function as well as how preprocessing to scale our features can aid our ML implementations. \n",
    "\n",
    "For this tutorial, read through and try to understand the text and code examples provided (ask your tutor if you have any questions) and then attempt the selection of exercises provided. Attempt these exercises on your own, but do ask your tutor for help if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSo3PZoZeF3M"
   },
   "source": [
    "## 0. Do not forget to import all the Python Libraries being used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DzwtdIweGv_"
   },
   "outputs": [],
   "source": [
    "import numpy as np # A useful package for dealing with mathematical processes, we will be using it this week for vectors and matrices\n",
    "import pandas as pd # A common package for viewing tabular data\n",
    "import tensorflow as tf # Tensor flow is a key package for performing automatic differntiation (the gradient descent we use for optimisation)\n",
    "import sklearn.linear_model, sklearn.datasets # We want to be able to access the sklearn datasets again, also we are using some model evaluation\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler # We will be using the imbuilt sclaing functions sklearn provides\n",
    "import matplotlib.pyplot as plt # We will be using Matplotlib for our graphs\n",
    "from sklearn.model_selection import train_test_split # A library that can automatically perform data splitting for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eoYSKIufcTJ"
   },
   "source": [
    "Reminder: To execute a cell, press SHIFT + ENTER or hit the 'play' button on the left of the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLr5ZOw5hror"
   },
   "source": [
    "# 1. Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lFnJS_ghIpJ"
   },
   "source": [
    "As explained in the lectures, our cost function is, as it's a name suggests, simply a mathematical function which relates the parameters of our model (the values in our $\\theta$ vector describing our hypothesis) to the error of the models predictions. Obviously we want to minimise this error, which we can do using optimisation.\n",
    "\n",
    "In order to formulate our optimisation problem I will start with some revision about defining a function and obtaining the function derivative, then showing how tensorflow can help us with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeck1Q85jC-P"
   },
   "source": [
    "## 1.1 Functions and differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8eWTE1J_bau"
   },
   "source": [
    "A function, at $J$ maps the elements in the set $\\theta$ to the set $y$ at it's most basic definition such that $J(\\theta) = y = a * \\theta^n$ (note, the more usual form you will see in mathematics is $f(x) = y = a * x^n$, but I am using our cost function notation for consistency). The derivative of this function at a given point is the slope of the tangent of the function at that point, which lets us know how that function is changing at that point.\n",
    "\n",
    "The differentiation function follows a lot of rules depending on the function you are trying to find the derivative of, so I will just give a few examples that will be helpful in this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPKRR4xOCCr7"
   },
   "source": [
    "### 1.1.1 Differentiation of a standard function with powers\n",
    "For a function of the form $J(\\theta) = y = a * \\theta^n$, the derivative of that function can be defined as \n",
    "$\\frac{d}{d\\theta} J(\\theta)= n \\times a \\theta^{n-1}$\n",
    "\n",
    "Example:\n",
    "\n",
    "Let $a=5$, $n=3$ and $\\theta=-2$.\n",
    "\n",
    "Define $J$ by $J(\\theta) = a \\theta^n = 5 \\theta^{3}$.\n",
    "\n",
    "Then: $J(-2) = 5 (-2)^3=-40$\n",
    "\n",
    "Find the derivative:\n",
    "\n",
    "$\\frac{d}{d\\theta} J(\\theta)= n \\times a \\times \\theta^{n-1} = 3 \\times 5 \\times \\theta^2=15\\theta^2$\n",
    "\n",
    "Value at $\\theta=-2$: $\\frac{d}{d\\theta} J(-2)= 15\\times -2^2 = 60$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1Mr4SaFIGeQ"
   },
   "source": [
    "### 1.1.2 Differentiation of a standard function with exponentials\n",
    "For a function of the form $J(\\theta) = y = \\exp (\\theta^n + a)$ (where $\\exp (x)$ is Napier's constant defined by $\\mathrm{e}^x$, where $\\mathrm{e}=2.71828...$ ), the derivative of that function can be defined as \n",
    "$\\frac{d}{d\\theta} J(\\theta)= n\\theta^{n-1} \\times \\exp (\\theta^n + a)$\n",
    "\n",
    "Example:\n",
    "\n",
    "Let $a=3$, $n=2$ and $\\theta=1$.\n",
    "\n",
    "Define $J$ by $J(\\theta) = \\exp (\\theta^n + a) = \\exp (\\theta^2 + 3)$.\n",
    "\n",
    "Then: $J(1) = \\exp (1^2 + 3)=54.59815$\n",
    "\n",
    "Find the derivative:\n",
    "\n",
    "$\\frac{d}{d\\theta} J(\\theta)= n\\theta^{n-1} \\times \\exp (\\theta^n + a) = 2\\theta \\times \\exp (\\theta^2 + 3)$\n",
    "\n",
    "Value at $\\theta=1$: $\\frac{d}{d\\theta} J(1)= 2\\times 1  \\times \\exp (1^2 + 3) = 2 \\exp (4) = 109.1963$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcXHY7wyB8sm"
   },
   "source": [
    "\n",
    "Functions and differention are fundamental topics in mathematics that you should have encountered before, so I won't go into any further detail here, but you should be able to find extensive examples of differentiation if you search for 'A level calculus' and 'differentiation rules' either in books or online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfX6E4xJp4Pu"
   },
   "source": [
    "## 1.2 Defining a function to differentiate in Python (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsC8dqblLXum"
   },
   "source": [
    "Define the \"variable\" (to which which we will calculate the derivative.)\n",
    "- `tf.Variable(value, dtype=np.float32)`\n",
    "  - `value` can be a python list, numpy array, or tensorflow Tensor, and will be regarded as the initial value.\n",
    "\n",
    "Define the \"constants\" (any variable for which we do not need to calculate the derivative)\n",
    "- `tf.constant(value, dtype=np.float32)`\n",
    "  - `value` is the value of the \"constant Tensor\"\n",
    "  - Strictly speaking, the \"Tensor\" defined by this is computationally immutable, so not \"constant.\"\n",
    "\n",
    "Note: Tensorflow gives us access to many mathematical functions we may have used prior with Numpy.\n",
    "- Operators are similar to those in Numpy\n",
    "  - `+, -, *, /`: elementwise addition $+$, subtraction $-$, multiplication $\\otimes$, division $\\oslash$.\n",
    "  - `@`: matrix product\n",
    "- Most mathmatical function can be given by replacing np of a Numpy function by tf.math.\n",
    "  - `np.sin(x), np.cos(x), np.exp(x)` $\\to$ `tf.math.sin(x), tf.math.cos(x), tf.math.exp(x)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KUhg85qqq3Q"
   },
   "source": [
    "## 1.3 How to differentiate a function automatically?\n",
    "The following is the general process we follow for automatically differentiating functions in Python: \n",
    "\n",
    "- Define the Variables (e.g., `th` for  $\\theta$) and any constants\n",
    "- Define return value (e.g., `j` for $J(\\theta)$ at some value for $\\theta$) of the function we want to differentiate in `tf.gradientTape() as tape:` block\n",
    "- Get the gradient by `tape.gradient()` (e.g., `tape.gradient(j, th)`)\n",
    "  - The 0th parameter: the return value instance (e.g., `j`).\n",
    "  - The 1st parameter: the variable instance (e.g., `th`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D60hTD2ZjT2t"
   },
   "source": [
    "### 1.3.1 Automatically differentiate of a standard function with powers\n",
    "Using the same function as in *1.1.1* - for a function of the form $J(\\theta) = y = a * \\theta^n$, the derivative of that function can be defined as \n",
    "$\\frac{d}{d\\theta} J(\\theta)= n \\times a \\theta^{n-1}$. \n",
    "Let $a=5$, $n=3$ and $\\theta=-2$, find the derivative $\\frac{d}{d\\theta} J(\\theta)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2eo5lcHjsf-",
    "outputId": "5478d9ff-b5a5-41e0-ec36-f581b96f18d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J(-2) = -40.0\n",
      "d/dth J(-2) = 60.0\n"
     ]
    }
   ],
   "source": [
    "th = tf.Variable(-2, dtype=np.float32) # Defining a variable `th`. Here, the initial value is -2. In the following, the derivative will be calculated for `th`.\n",
    "a = tf.constant(5, dtype=np.float32) # Defining a constant `a`. Here, the value is set to 5. \n",
    "n = tf.constant(3, dtype=np.float32) # Defining a constant `a`. Here, the value is set to 5. \n",
    "\n",
    "def j_func(th): # It is convenient to define the function we want to differentiate before the `tf.GradientTape` block. \n",
    "  j = a * (th ** n)\n",
    "  return j\n",
    "\n",
    "with tf.GradientTape() as tape: # The gradienttape function allows us to track the tensorflow computations which it uses to calculate the gradients with respect to our `th`\n",
    "  j = j_func(th) # Using `j_func` function defined above, we define the return value of the function.\n",
    "\n",
    "# What is the value of our function at `th=-2`?\n",
    "print('J(-2) =', j.numpy()) # To convert an TensorFlow instance to a NumPy instance, we use the `numpy()` method.\n",
    "\n",
    "# What is the derivative of our function at `th=-2`?\n",
    "# tape.gradient(j, th) takes the return value `j` and the variable value `th` to give the derivative at that point.\n",
    "dj_dth = tape.gradient(j, th) # To calculate the derivative of the function, we use `tape.gradient` method.\n",
    "print('d/dth J(-2) =', dj_dth.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjDlkSqfR3UN"
   },
   "source": [
    "We should be able to see this matches our example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVN-xvgfmYHA"
   },
   "source": [
    "### 1.3.2 Automatically differentiate of a standard function with exponentials\n",
    "Using the same function as in *1.1.2* - for a function of the form $J(\\theta) = y = \\exp (\\theta^n + a)$, the derivative of that function can be defined as \n",
    "$\\frac{d}{d\\theta} J(\\theta)= n\\theta^{n-1} \\times \\exp (\\theta^n + a)$. \n",
    "Let $a=3$, $n=2$ and $\\theta=1$, find the derivative $\\frac{d}{d\\theta} J(\\theta)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OG7-llzrbac6"
   },
   "outputs": [],
   "source": [
    "th = tf.Variable(1, dtype=np.float32) # Defining a variable `th`. Here, the initial value is 1. In the following, the derivative will be calculated for `th`.\n",
    "a = tf.constant(3, dtype=np.float32) # Defining a constant `a`. Here, the value is set to 3. \n",
    "\n",
    "def j_func(th): # It is convenient to define the function we want to differentiate before the `tf.GradientTape` block. \n",
    "  j = tf.math.exp(th ** 2 + a) # We use tf.math.exp to implement the exponential function.\n",
    "  return j\n",
    "\n",
    "with tf.GradientTape() as tape: # The gradienttape function allows us to track the tensorflow computations which it uses to calculate the gradients with respect to our `th`\n",
    "  j = j_func(th) # Using `j_func` function defined above, we define the return value of the function.\n",
    "\n",
    "# What is the value of our function at `th=-2`?\n",
    "print('J(1) =', j.numpy()) # To convert an TensorFlow instance to a NumPy instance, we use the `numpy()` method.\n",
    "\n",
    "# What is the derivative of our function at `th=-2`?\n",
    "# tape.gradient(j, th) takes the return value `j` and the variable value `th` to give the derivative at that point.\n",
    "dj_dth = tape.gradient(j, th) # To calculate the derivative of the function, we use `tape.gradient` method.\n",
    "print('d/dth J(1) =', dj_dth.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTlTc6enl0qz"
   },
   "source": [
    "### 1.3.3 Task 1: Differentiate the sigmoid function\n",
    "Consider the sigmoid function (which you will be seeing a lot more of later) which takes the form:\n",
    "$$J(\\theta) = \\frac{1}{1 + \\exp (- a \\theta)}$$\n",
    "\n",
    "\n",
    "Let $a=1$ and $\\theta=0$, find the derivative $\\frac{d}{d\\theta} J(\\theta)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVo9G_xsmqVd",
    "outputId": "6bcce915-7afc-42d8-92d4-83f3580c4956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J(0) = 0.5\n",
      "d/dth J(0) = 0.25\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "\n",
    "th = tf.Variable(0.0, dtype=np.float32)\n",
    "a = tf.constant(1.0, dtype=np.float32)\n",
    "def j_func(th):\n",
    "  j = 1.0 / (1.0 + tf.math.exp(- a * th))\n",
    "  return j\n",
    "\n",
    "with tf.GradientTape() as tape: # The gradienttape function allows us to track the tensorflow computations which it uses to calculate the gradients with respect to our `th`\n",
    "  j = j_func(th) # Using `j_func` function defined above, we define the return value of the function.\n",
    "\n",
    "# What is the value of our function at `th=-2`?\n",
    "print('J(0) =', j.numpy()) # To convert an TensorFlow instance to a NumPy instance, we use the `numpy()` method.\n",
    "\n",
    "# What is the derivative of our function at `th=-2`?\n",
    "# tape.gradient(j, th) takes the return value `j` and the variable value `th` to give the derivative at that point.\n",
    "dj_dth = tape.gradient(j, th) # To calculate the derivative of the function, we use `tape.gradient` method.\n",
    "print('d/dth J(0) =', dj_dth.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pMUdf9QUvES"
   },
   "source": [
    "If your code working correctly your code should give you the results: $J(0)=0.5$ and $\\frac{d}{d\\theta} J(0)=0.25$.\n",
    "\n",
    "Though if you want to do some maths to double check this and differentiate the sigmoid function by hand be my guest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbcyADjmg-FS"
   },
   "source": [
    "## 1.4 Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWQGNuvBUb6g"
   },
   "source": [
    "The objective of the gradient descent method is to find the vector $\\boldsymbol{\\theta}$ which minimises our function $J(\\boldsymbol{\\theta})$. It does this by calcuating the derivative at a point on the function then updating the position to closer to the minimum based on the gradient and the user defined `learning rate`, hence the implementation required a loop that will calculate the derivative and update $\\theta$ at multiple points.\n",
    "\n",
    "Algorithm\n",
    "- Inputs: \n",
    "  - The initial parameter guess $\\boldsymbol{\\theta}_\\textrm{init}$ (note: ideally we would want to choose values close to the minimum - but if we knew what that was that we wouldn't be needing to use this method! Consequently, you generally start by intiialisng the vector just with zeros or ones)\n",
    "  - The Learning rate $\\alpha$. This must take a value of $\\alpha >0$ and is generally less than $1$, but you have to experiment to find the best value for this hyperparameter.\n",
    "- Initialization: Set your parameter vector $\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_\\textrm{init}$.\n",
    "- Repeat the following steps until convergence (when $\\boldsymbol{\\theta}$ stops changing):\n",
    "  - Calculate the value of the derviative of the function at the current  $\\boldsymbol{\\theta}$ values and save this as the variabe $\\boldsymbol{g} =\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J (\\boldsymbol{\\theta})$ \n",
    "  - Update your estimate of $\\boldsymbol{\\theta}$ using $\\boldsymbol{g}$ and $\\alpha$ using: $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\alpha \\boldsymbol{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mgpQz-wi2wR"
   },
   "source": [
    "### 1.4.1 How to apply the gradient descent algorithm in TensorFlow\n",
    "**Note**: The following explains low-level interfaces of TensorFlow. For machine learning problems, you can also use high-level interfaces of TensorFlow, which will be explained in following weeks for use with Neural Networks, which does a lot of this hard work for you behind the scenes.\n",
    "- Create `tf.optimizers.SGD` instance.\n",
    "  - inputs: learning rate $\\alpha$.\n",
    "- For each step in the loop, get the gradient using `tape.gradient` and update the parameters using `optimizers.apply_gradients` from our SGD instance.\n",
    "  - inputs: the sequence of pairs (tuples of length 2) of the `tf.Variable` instance and the gradient given by `tape.gradient` method for that iteration\n",
    "\n",
    "Hopefully the commented example below will make things clearer if you don't quite understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7QN3DeiUexS"
   },
   "source": [
    "### 1.4.1 SGD algorithm Example\n",
    "For our problem we want to find the $\\theta$ that minimizes the function $J (\\theta)$.\n",
    "\n",
    "Let:\n",
    "- $J (\\theta) = - \\exp(- (\\theta - a)^2)$, where $a = 0.5$\n",
    "- An initial guess $\\theta_\\textrm{init} = 0.0$\n",
    "- A learning rate $\\alpha = 0.1$\n",
    "\n",
    "(The solution to this problem should be $J (\\theta)$ takes the minimum value $-1.0$ at $\\theta = 0.5$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpA8Gu5UdC-L",
    "outputId": "f7c45e5e-7c66-4fce-eccf-04ac02b3b1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00: j = -0.7788, th = +0.0779, dj/dth = -0.7788\n",
      "step 01: j = -0.8368, th = +0.1485, dj/dth = -0.7064\n",
      "step 02: j = -0.8838, th = +0.2107, dj/dth = -0.6213\n",
      "step 03: j = -0.9197, th = +0.2639, dj/dth = -0.5322\n",
      "step 04: j = -0.9458, th = +0.3085, dj/dth = -0.4466\n",
      "step 05: j = -0.9640, th = +0.3455, dj/dth = -0.3691\n",
      "step 06: j = -0.9764, th = +0.3756, dj/dth = -0.3018\n",
      "step 07: j = -0.9847, th = +0.4001, dj/dth = -0.2449\n",
      "step 08: j = -0.9901, th = +0.4199, dj/dth = -0.1978\n",
      "step 09: j = -0.9936, th = +0.4358, dj/dth = -0.1592\n",
      "step 10: j = -0.9959, th = +0.4486, dj/dth = -0.1278\n",
      "step 11: j = -0.9974, th = +0.4589, dj/dth = -0.1025\n",
      "step 12: j = -0.9983, th = +0.4671, dj/dth = -0.0822\n",
      "step 13: j = -0.9989, th = +0.4736, dj/dth = -0.0658\n",
      "step 14: j = -0.9993, th = +0.4789, dj/dth = -0.0527\n",
      "step 15: j = -0.9996, th = +0.4831, dj/dth = -0.0422\n",
      "step 16: j = -0.9997, th = +0.4865, dj/dth = -0.0337\n",
      "step 17: j = -0.9998, th = +0.4892, dj/dth = -0.0270\n",
      "step 18: j = -0.9999, th = +0.4914, dj/dth = -0.0216\n",
      "step 19: j = -0.9999, th = +0.4931, dj/dth = -0.0173\n",
      "step 20: j = -1.0000, th = +0.4945, dj/dth = -0.0138\n",
      "step 21: j = -1.0000, th = +0.4956, dj/dth = -0.0111\n",
      "step 22: j = -1.0000, th = +0.4965, dj/dth = -0.0088\n",
      "step 23: j = -1.0000, th = +0.4972, dj/dth = -0.0071\n",
      "step 24: j = -1.0000, th = +0.4977, dj/dth = -0.0057\n",
      "step 25: j = -1.0000, th = +0.4982, dj/dth = -0.0045\n",
      "step 26: j = -1.0000, th = +0.4986, dj/dth = -0.0036\n",
      "step 27: j = -1.0000, th = +0.4988, dj/dth = -0.0029\n",
      "step 28: j = -1.0000, th = +0.4991, dj/dth = -0.0023\n",
      "step 29: j = -1.0000, th = +0.4993, dj/dth = -0.0019\n",
      "Final results: j = -1.0000, th = +0.4993\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "th = tf.Variable(0, dtype=np.float32) # Defining a variable `th`, this is our `initial guess`\n",
    "a = tf.constant(0.5, dtype=np.float32) # Defining a constant `a`. Here, the value is set to 5. \n",
    "\n",
    "def j_func(th): # It is convenient to define the function we want to differentiate before the `tf.GradientTape` block. \n",
    "  j = - tf.math.exp(- (th - a) ** 2)\n",
    "  return j\n",
    "\n",
    "learning_rate = 0.1 # Our `alpha` which dictates how much our values can change in a single step.\n",
    "optimizer = tf.optimizers.SGD(learning_rate) # creating our optimiser object\n",
    "n_steps = 30 # Numer of steps we want the SGD to run\n",
    "display_interval = 1 # How often we want to print our outputs (for this case printing every iteration is fine, but you would not want to do this if you weere running thousands of iterations)\n",
    "\n",
    "for i_step in range(n_steps): # Start loop running from 1-n_steps\n",
    "  with tf.GradientTape() as tape: # The gradienttape function allows us to track the tensorflow computations which it uses to calculate the gradients with respect to our `th`\n",
    "    j = j_func(th) # Using `j_func` function defined above, we define the return value of the function.\n",
    "\n",
    "  dj_dth = tape.gradient(j, th) # Getting the gradient at current theta\n",
    "\n",
    "  #Note: zip() function combines [dj_dth], [th] into a zipped tuple form the optimiser function needs\n",
    "  optimizer.apply_gradients(zip([dj_dth], [th])) # Updating `th` using the gradient `dj_dth`.\n",
    "\n",
    "  # print the current status\n",
    "  if i_step % display_interval == 0:\n",
    "    print(f'step {i_step:0=2}: j = {j.numpy():+2.4f}, th = {th.numpy():+2.4f}, dj/dth = {dj_dth.numpy():+2.4f}')\n",
    "\n",
    "# print the final results\n",
    "j = j_func(th)\n",
    "print(f'Final results: j = {j.numpy():+2.4f}, th = {th.numpy():+2.4f}')\n",
    "# Note print(f'....') allows us to format any variables being called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g71zGXYPiakN"
   },
   "source": [
    "### 1.4.2 Task 2\n",
    "Now try applying the SGD yourself to find the $\\theta$ that minimizes the function $J (\\theta)$.\n",
    "\n",
    "Let:\n",
    "- $J (\\theta) = \\frac{a^2}{\\theta} + \\theta$, where $a = 0.5$\n",
    "- An initial guess $\\theta_\\textrm{init} = 1.0$\n",
    "- A learning rate $\\alpha = 0.1$\n",
    "\n",
    "(The solution to this problem should be $J (\\theta)$ takes the minimum value $1.0$ at $\\theta = 0.5$)\n",
    "\n",
    "To understand the algorithm better, also try altering the following and see how it effects the algorithm:\n",
    "*   Change the initial guess $\\theta_\\textrm{init}$ to be closer and further away from the true answer (you might find something interesting if you move too far away...)\n",
    "*   Try other values of $\\alpha$ (remember it must be greater than 0)\n",
    "*   Try changing the number of steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWlbwlBthUrH",
    "outputId": "601c3ec1-7074-426c-c4a0-b484d42496b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00: j = +1.2500, th = +0.9250, dj/dth = +0.7500\n",
      "step 01: j = +1.1953, th = +0.8542, dj/dth = +0.7078\n",
      "step 02: j = +1.1469, th = +0.7885, dj/dth = +0.6574\n",
      "step 03: j = +1.1055, th = +0.7287, dj/dth = +0.5979\n",
      "step 04: j = +1.0718, th = +0.6758, dj/dth = +0.5292\n",
      "step 05: j = +1.0457, th = +0.6305, dj/dth = +0.4526\n",
      "step 06: j = +1.0270, th = +0.5934, dj/dth = +0.3712\n",
      "step 07: j = +1.0147, th = +0.5644, dj/dth = +0.2900\n",
      "step 08: j = +1.0073, th = +0.5429, dj/dth = +0.2152\n",
      "step 09: j = +1.0034, th = +0.5277, dj/dth = +0.1517\n",
      "step 10: j = +1.0015, th = +0.5175, dj/dth = +0.1023\n",
      "step 11: j = +1.0006, th = +0.5108, dj/dth = +0.0664\n",
      "step 12: j = +1.0002, th = +0.5066, dj/dth = +0.0420\n",
      "step 13: j = +1.0001, th = +0.5040, dj/dth = +0.0260\n",
      "step 14: j = +1.0000, th = +0.5024, dj/dth = +0.0160\n",
      "step 15: j = +1.0000, th = +0.5015, dj/dth = +0.0097\n",
      "step 16: j = +1.0000, th = +0.5009, dj/dth = +0.0059\n",
      "step 17: j = +1.0000, th = +0.5005, dj/dth = +0.0035\n",
      "step 18: j = +1.0000, th = +0.5003, dj/dth = +0.0021\n",
      "step 19: j = +1.0000, th = +0.5002, dj/dth = +0.0013\n",
      "step 20: j = +1.0000, th = +0.5001, dj/dth = +0.0008\n",
      "step 21: j = +1.0000, th = +0.5001, dj/dth = +0.0005\n",
      "step 22: j = +1.0000, th = +0.5000, dj/dth = +0.0003\n",
      "step 23: j = +1.0000, th = +0.5000, dj/dth = +0.0002\n",
      "step 24: j = +1.0000, th = +0.5000, dj/dth = +0.0001\n",
      "step 25: j = +1.0000, th = +0.5000, dj/dth = +0.0001\n",
      "step 26: j = +1.0000, th = +0.5000, dj/dth = +0.0000\n",
      "step 27: j = +1.0000, th = +0.5000, dj/dth = +0.0000\n",
      "step 28: j = +1.0000, th = +0.5000, dj/dth = +0.0000\n",
      "step 29: j = +1.0000, th = +0.5000, dj/dth = +0.0000\n",
      "Final results: j = +1.0000, th = +0.5000\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "\n",
    "# Define the objective function\n",
    "th = tf.Variable(1, dtype=np.float32) # Defining a variable `th`, this is our `initial guess`\n",
    "a = tf.constant(0.5, dtype=np.float32) # Defining a constant `a`. Here, the value is set to 5. \n",
    "\n",
    "def j_func(th): # It is convenient to define the function we want to differentiate before the `tf.GradientTape` block. \n",
    "  j = ((a ** 2) / th) + th\n",
    "  return j\n",
    "\n",
    "learning_rate = 0.1 # Our `alpha` which dictates how much our values can change in a single step.\n",
    "optimizer = tf.optimizers.SGD(learning_rate) # creating our optimiser object\n",
    "n_steps = 30 # Numer of steps we want the SGD to run\n",
    "display_interval = 1 # How often we want to print our outputs (for this case printing every iteration is fine, but you would not want to do this if you weere running thousands of iterations)\n",
    "\n",
    "for i_step in range(n_steps): # Start loop running from 1-n_steps\n",
    "  with tf.GradientTape() as tape: # The gradienttape function allows us to track the tensorflow computations which it uses to calculate the gradients with respect to our `th`\n",
    "    j = j_func(th) # Using `j_func` function defined above, we define the return value of the function.\n",
    "\n",
    "  dj_dth = tape.gradient(j, th) # Getting the gradient at current theta\n",
    "\n",
    "  #Note: zip() function combines [dj_dth], [th] into a zipped tuple form the optimiser function needs\n",
    "  optimizer.apply_gradients(zip([dj_dth], [th])) # Updating `th` using the gradient `dj_dth`.\n",
    "\n",
    "  # print the current status\n",
    "  if i_step % display_interval == 0:\n",
    "    print(f'step {i_step:0=2}: j = {j.numpy():+2.4f}, th = {th.numpy():+2.4f}, dj/dth = {dj_dth.numpy():+2.4f}')\n",
    "\n",
    "# print the final results\n",
    "j = j_func(th)\n",
    "print(f'Final results: j = {j.numpy():+2.4f}, th = {th.numpy():+2.4f}')\n",
    "# Note print(f'....') allows us to format any variables being called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk3TSyKvcHp_"
   },
   "source": [
    "## 1.5 Using SGD for functions with multiple parameters using vectors/matrices\n",
    "So far we have only been looking at how to optimise functions with a single parameter, however in reality we are mostly going to be using multivariable functions.\n",
    "We can account for this by defining vectors and matrices as `tf.Variable` and `tf.constant` instances where we input a list or a NumPy array when creating the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKes1VbNTku4"
   },
   "source": [
    "##1.5.1 Two Parameter Example \n",
    "\n",
    "For a function $J(\\boldsymbol{\\theta}) = \\sum_{i=0,1}(\\theta_{i} - c_{i})^2 =\\theta_{0}^2 -2c_{0}\\theta_{0} + c_{0}^2 +\\theta_{1}^2 -2c_{1}\\theta_{1} + c_{1}^2 = (\\boldsymbol{\\theta} - \\boldsymbol{c})^\\top (\\boldsymbol{\\theta} - \\boldsymbol{c})$, let our constant vector $\\boldsymbol{c} = \\begin{bmatrix}2 \\\\ 3\\end{bmatrix}$ and our parameter vector be $\\boldsymbol{\\theta}  = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Find:\n",
    "\n",
    "\n",
    "*   $ J \\left(\\begin{bmatrix}5 \\\\ 4\\end{bmatrix}\\right)$\n",
    "*   $\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J \\left(\\begin{bmatrix}5 \\\\ 4\\end{bmatrix}\\right) $\n",
    "\n",
    "\n",
    "Solution:\n",
    "- $J \\left(\\begin{bmatrix}5 \\\\ 4\\end{bmatrix}\\right) =5^2 -2\\times 2 \\times 5 + 2^2 + 4^2 -2\\times 3 \\times 4 + 3^2= 10$\n",
    "- $\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J (\\boldsymbol{\\theta}) = 2 (\\boldsymbol{\\theta} - \\boldsymbol{c})$ (note: this differentiation involves the chain rule, don't worry if you don't fully follow how this worked).\n",
    "- $\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J \\left(\\begin{bmatrix}5 \\\\ 4\\end{bmatrix}\\right) = 2 \\left(\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} - \\begin{bmatrix}2 \\\\ 3\\end{bmatrix}\\right) = \\begin{bmatrix}6 \\\\ 2\\end{bmatrix}.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82I1RI3ifZhy"
   },
   "outputs": [],
   "source": [
    "# Then solve using Python\n",
    "th_by_list = [[5],[4]] # Define our list of parameters\n",
    "c_by_list = [[2],[3]] # Define our list of constants\n",
    "\n",
    "# Make both into TensorFLow objects\n",
    "th = tf.Variable(th_by_list, dtype=np.float32)\n",
    "c = tf.constant(c_by_list, dtype=np.float32)\n",
    "\n",
    "\n",
    "def j_func(th): # It is convenient to define the function we want to differentiate before the `tf.GradientTape` block. \n",
    "  j = tf.transpose(th - c) @ (th - c) # (th - c).T does not work in Tensorflow, and remember that `@` is how TF does matrix multiplication.\n",
    "  return j\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  j = j_func(th)\n",
    "\n",
    "\n",
    "dj_dth = tape.gradient(j, th)\n",
    "print('j =')\n",
    "print(j.numpy())\n",
    "print('dj/dth =')\n",
    "print(dj_dth.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "577GnIWU1ilL"
   },
   "source": [
    "Now we have shown how to differentiate this function, we can also now use the SGD method to find the values of $\\boldsymbol{\\theta}$ that minimise $J(\\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NKNOiRQq_Hj"
   },
   "outputs": [],
   "source": [
    "\n",
    "th_by_list = [[5],[4]] # Define our list of parameters\n",
    "c_by_list = [[2],[3]] # Define our list of constants\n",
    "\n",
    "# Make both into TensorFLow objects\n",
    "th = tf.Variable(th_by_list, dtype=np.float32)\n",
    "c = tf.constant(c_by_list, dtype=np.float32)\n",
    "\n",
    "def j_func(th): # It is convenient to define the function we want to differentiate before the `tf.GradientTape` block. \n",
    "  j = tf.transpose(th - c) @ (th - c) # (th - c).T does not work in Tensorflow, and remember that `@` is how TF does matrix multiplication.\n",
    "  return j\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "n_steps = 80\n",
    "display_interval = 10 # The output with vectors is quite long and ugly, so I have increased the display interval\n",
    "for i_step in range(n_steps): \n",
    "  with tf.GradientTape() as tape:\n",
    "    j = j_func(th)\n",
    "\n",
    "  dj_dth = tape.gradient(j, th) # get gradient\n",
    "\n",
    "  optimizer.apply_gradients(zip([dj_dth], [th])) # update using gradient\n",
    "\n",
    "  # print the current status\n",
    "  if i_step % display_interval == 0:\n",
    "    print(f'step {i_step:0=2}: \\nj = \\n{j.numpy()}, \\nth = \\n{th.numpy()}, \\ndj/dth = \\n{dj_dth.numpy()} \\n')\n",
    "\n",
    "# print the final results\n",
    "j = j_func(th)\n",
    "print(f'Final results: j = {j.numpy()}, th = {th.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ551J8Mzk-T"
   },
   "source": [
    "# 2. Optimisation in machine learning\n",
    "\n",
    "So far we have been looking at optimisation in the somewhat abstract realm of mathematics, but what we really care about is how we can apply this to our ML models to find the best hypothesis.\n",
    "\n",
    "These days, most famous machine learning models are generally already implemented in machine learning libraries such as `sklearn`, which makes out lives a lot easier as elements such as the optimisation have already been implemented. However, we sometimes may find a good model in a recent academic paper that has not been implemented in those libraries - in that case, we will have to implement the cost function of the model and optimise it by ourselves.\n",
    "\n",
    "\n",
    "In the following section we will practice implementing a machine learning model and its optimisation from scratch (Note: this will be a multivariate model, which I have mentioned previously, but will explain in much more detail next week)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDutRdmkzveS"
   },
   "source": [
    "## 2.1 Step 1 - Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvH6SJ9_5EgA"
   },
   "source": [
    "You should all hopefully be quite familliar with loading data from sklearn now, in particular the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MdCIgnqEzpho",
    "outputId": "6a51ba45-2287-481b-91e8-50efcc8eb0d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-aa9ffeee-4d4d-4e37-ab70-941483b4cd34\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa9ffeee-4d4d-4e37-ab70-941483b4cd34')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-aa9ffeee-4d4d-4e37-ab70-941483b4cd34 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-aa9ffeee-4d4d-4e37-ab70-941483b4cd34');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  MedHouseVal  \n",
       "0        -122.23        4.526  \n",
       "1        -122.22        3.585  \n",
       "2        -122.24        3.521  \n",
       "3        -122.25        3.413  \n",
       "4        -122.25        3.422  \n",
       "...          ...          ...  \n",
       "20635    -121.09        0.781  \n",
       "20636    -121.21        0.771  \n",
       "20637    -121.22        0.923  \n",
       "20638    -121.32        0.847  \n",
       "20639    -121.24        0.894  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the house price dataset\n",
    "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "\n",
    "# View the data\n",
    "display(pd.concat([X_pd, y_pd], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k97ocXBY6WA5"
   },
   "source": [
    "## 2.2 Step 2 - Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkVrgClBEWSh"
   },
   "source": [
    "Convert our data into arrays and use the sklearn `train_test_split` function to split our data into two sets - one for training and one for evaluation (we will explain the theory behind this in a later week). Note that we are only using a small subset of the data for speed of 'training'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDbB0ixO1S7w"
   },
   "outputs": [],
   "source": [
    "# Convert Pandas dataframes to numpy arrays (that our ML models need)\n",
    "X = X_pd.to_numpy()\n",
    "y = y_pd.to_numpy() \n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.08, test_size=0.02, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDVUME-H28hh"
   },
   "source": [
    "## 2.3 Step 3 - Data Pre-processing\n",
    "As described in the lecture, feature scaling has benefits for our ML implementation, one of which is better convergence behaviour for the SGD method.\n",
    "\n",
    "The two main methods we currently care about are Standaisation and Normalisation (MinMax Scaling), both of which are easy to apply using sklearn.\n",
    "\n",
    "\n",
    "### Standardization\n",
    "By calculating the mean and variance for our data columns we can scale each feature so that it has a mean of 0 and a variance of 1.\n",
    "$$x_s = \\frac{x-\\mu_x}{\\sigma_x}$$\n",
    "We can standardize the data by the `sklearn.preprocessing.StandardScaler` instance.\n",
    "- Initialization: `StandardScaler` initializer.\n",
    "- Calculating the mean and variance (standard deviation) by using `fit` with our object and data.\n",
    "  - Input: the `np.array` instance of which to calculate the mean and variance.\n",
    "- Apply the standardization using `transform`.\n",
    "\n",
    "### Normalisation (MinMax)\n",
    "By calculating the maximum and minimum values for our data columns we can scale each feature so that it takes a value between 0 and 1.\n",
    "$$x_s = \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "We can standardize the data by the `sklearn.preprocessing.MinMaxScaler` instance.\n",
    "- Initialization: `MinMaxScaler` initializer.\n",
    "- Calculating the mean and variance (standard deviation) by using `fit` with our object and data.\n",
    "  - Input: the `np.array` instance of which to calculate the mean and variance.\n",
    "- Apply the standardization using `transform`.\n",
    "\n",
    "For our case we will apply standardisation, which is generally a better choice for unbounded data like we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWUmxKYu1p4L",
    "outputId": "719a0ed0-1d45-4522-b0b9-2da3c34b19ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = \n",
      "[ 3.86941211e+00  2.89085403e+01  5.40527659e+00  1.09310084e+00\n",
      "  1.40576620e+03  3.66859463e+00  3.56305512e+01 -1.19553398e+02], \n",
      "var = \n",
      "[3.89567163e+00 1.56857171e+02 5.01194690e+00 1.47144384e-01\n",
      " 9.99838189e+05 9.32007571e+02 4.61078982e+00 3.98138985e+00]\n",
      "mean = \n",
      "[-1.14505619e-15  5.93777669e-17  3.57769538e-15  3.82929438e-16\n",
      "  5.27204634e-17  4.17636516e-17  9.98797249e-15 -6.73240318e-14], \n",
      "var = \n",
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # calculate the mean and variance for each feature and store to attributes\n",
    "print(f'mean = \\n{scaler.mean_}, \\nvar = \\n{scaler.var_}') # print the calculated mean and variance for each attribute\n",
    "X_train_stded = scaler.transform(X_train) # standardize X_train\n",
    "print(f'mean = \\n{np.mean(X_train_stded, axis=0)}, \\nvar = \\n{np.var(X_train_stded, axis=0)}') # verify that X_train_stded has mean 0 (mean isn't quite 0 due to numerical error, but is a miniscule value) and variance 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKwEtRMH7ncw"
   },
   "source": [
    "### Append a column with 1s. \n",
    "As some of you may have noticed last week when implementing Linear Regression with sklearn, the model has (at least) two paramters, the constant 'intercept' and the main coefficients. Sklearn is smart and can ccount for the intercept existing for us, but as we are now fitting the model manually we need to have a extra column of '1s' to account for our intercept ($\\theta_0$). We can apply a function to do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0_RYk8972hy"
   },
   "outputs": [],
   "source": [
    "# Create 1 appending function\n",
    "def append_one_to(X_without_one):\n",
    "  X_with_one = np.pad(X_without_one, ((0, 0), (1, 0)), constant_values=1)\n",
    "  return X_with_one\n",
    "\n",
    "# Use this function to update our training data\n",
    "X_train_stded_with_one = append_one_to(X_train_stded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMgL86dRqEoJ"
   },
   "source": [
    "##2.4 Step 4: Optimising the Cost function - Mean Squared Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4u9qOFqhu9W"
   },
   "source": [
    "### 2.4.1 Define the mean squared error using TensorFlow\n",
    "As a quick reminder, the Cost function we are using currently is the Mean Squared Error (MSE), we are defining this as follows:\n",
    "$$J(\\boldsymbol{\\theta}, \\boldsymbol{X}, \\boldsymbol{y}) = \\frac{1}{m} \\sum_{i=0}^{m-1} (\\boldsymbol{X}_{i} \\boldsymbol{\\theta} - \\boldsymbol{y}_{i})^2 = \\frac{1}{m}(\\boldsymbol{X} \\boldsymbol{\\theta} - \\boldsymbol{y})^\\top (\\boldsymbol{X} \\boldsymbol{\\theta} - \\boldsymbol{y})$$\n",
    "\n",
    "\n",
    "where $X$ is our matrix of training feature data; $\\boldsymbol{y}$ are the observed predicted target values for the target data; $\\boldsymbol{\\theta}$ is the parameter vector (hypothesis); $m$ is the number of rows in our training data and a subscript of $i$ indicates we are using the $i$th row of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_yP-1LvESUq"
   },
   "source": [
    "### 2.4.2 Task 3 - Implement the the mean squared error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltDUCGGXjJhO"
   },
   "source": [
    "For this task I just want you to alter one line to define the MSE correctly using tensorflow to match the definition above. I have an answer hidden in a cell below as we can't contune the tutorial without it, but do attempt to work it out on your own first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtBEMdGJoMcc"
   },
   "outputs": [],
   "source": [
    "X = tf.constant(X_train_stded_with_one, dtype=np.float32) # Note, we can put a matrix in as a vector\n",
    "y = tf.constant(y_train[:, np.newaxis], dtype=np.float32) # We use y as a column vector, that is, m x 1 2D array, not an 1D array. For this reason, we input y_train[:, np.newaxis]\n",
    "m, n = X_train_stded_with_one.shape # We use the shape function to obtiam our number of rows - `m`\n",
    "th = tf.Variable(tf.zeros([n, 1], dtype=np.float32)) # Create a vector of zeroes for our initial guess\n",
    "\n",
    "def j_func(th):\n",
    "  ####### Your code Here ###################\n",
    "  j = 0 # Alter this J to calculate the MSE\n",
    "  j = (1. / m) * (tf.transpose(X @ th - y) @ (X @ th - y))\n",
    "  ####### end Code ###################\n",
    "  j = tf.squeeze(j) # the `squeeze` function takes our 1x1 matrix and turns it into a scalar value.\n",
    "  return j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLKHEc3gjquo"
   },
   "source": [
    "#### Answer: Don't look without trying it first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AjUcMc9jtaV"
   },
   "outputs": [],
   "source": [
    "# Don't look until you've tried it first!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#j = (1. / m) * (tf.transpose(X @ th - y) @ (X @ th - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFZrd3GC7c0h"
   },
   "source": [
    "### 2.4.3 Use the Steepest Gradient method to optimise your cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfiUCXvtXihi"
   },
   "source": [
    "Now we have our data and cost function correctly applied, the application of Steepest gradient is the same as when solving any other function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBot5QsOqar6",
    "outputId": "18512d33-c80d-43d0-c9e9-5cc89fe3c462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: j = 5.581369876861572\n",
      "Step 1: j = 3.7893619537353516\n",
      "Step 2: j = 2.6512207984924316\n",
      "Step 3: j = 1.9250277280807495\n",
      "Step 4: j = 1.4598783254623413\n",
      "Step 5: j = 1.160803198814392\n",
      "Step 6: j = 0.9676728248596191\n",
      "Step 7: j = 0.8422714471817017\n",
      "Step 8: j = 0.7602470517158508\n",
      "Step 9: j = 0.7060568928718567\n",
      "Step 10: j = 0.6697632074356079\n",
      "Step 11: j = 0.6450052857398987\n",
      "Step 12: j = 0.6277096271514893\n",
      "Step 13: j = 0.6152610182762146\n",
      "Step 14: j = 0.6059798002243042\n",
      "Step 15: j = 0.5987846255302429\n",
      "Step 16: j = 0.5929818153381348\n",
      "Step 17: j = 0.5881181955337524\n",
      "Step 18: j = 0.5839032530784607\n",
      "Step 19: j = 0.5801476240158081\n",
      "Step 20: j = 0.5767254829406738\n",
      "Step 21: j = 0.5735552310943604\n",
      "Step 22: j = 0.5705822110176086\n",
      "Step 23: j = 0.5677697658538818\n",
      "Step 24: j = 0.5650932788848877\n",
      "Step 25: j = 0.562533974647522\n",
      "Step 26: j = 0.5600805878639221\n",
      "Step 27: j = 0.5577225089073181\n",
      "Step 28: j = 0.5554530024528503\n",
      "Step 29: j = 0.5532671213150024\n",
      "Step 30: j = 0.5511600375175476\n",
      "Step 31: j = 0.5491275191307068\n",
      "Step 32: j = 0.5471661686897278\n",
      "Step 33: j = 0.5452722311019897\n",
      "Step 34: j = 0.5434441566467285\n",
      "Step 35: j = 0.5416793823242188\n",
      "Step 36: j = 0.5399761199951172\n",
      "Step 37: j = 0.5383293032646179\n",
      "Step 38: j = 0.5367403030395508\n",
      "Step 39: j = 0.5352053046226501\n",
      "Step 40: j = 0.5337225198745728\n",
      "Step 41: j = 0.5322893261909485\n",
      "Step 42: j = 0.530906081199646\n",
      "Step 43: j = 0.5295693278312683\n",
      "Step 44: j = 0.5282778143882751\n",
      "Step 45: j = 0.5270306468009949\n",
      "Step 46: j = 0.5258251428604126\n",
      "Step 47: j = 0.5246610045433044\n",
      "Step 48: j = 0.5235364437103271\n",
      "Step 49: j = 0.5224495530128479\n",
      "Step 50: j = 0.5213994383811951\n",
      "Step 51: j = 0.5203842520713806\n",
      "Step 52: j = 0.519404947757721\n",
      "Step 53: j = 0.5184573531150818\n",
      "Step 54: j = 0.517542839050293\n",
      "Step 55: j = 0.5166586637496948\n",
      "Step 56: j = 0.51580411195755\n",
      "Step 57: j = 0.5149786472320557\n",
      "Step 58: j = 0.5141806602478027\n",
      "Step 59: j = 0.5134096145629883\n",
      "Step 60: j = 0.5126648545265198\n",
      "Step 61: j = 0.5119452476501465\n",
      "Step 62: j = 0.511249840259552\n",
      "Step 63: j = 0.5105767846107483\n",
      "Step 64: j = 0.5099272131919861\n",
      "Step 65: j = 0.509299635887146\n",
      "Step 66: j = 0.5086925625801086\n",
      "Step 67: j = 0.5081064701080322\n",
      "Step 68: j = 0.5075397491455078\n",
      "Step 69: j = 0.5069912075996399\n",
      "Step 70: j = 0.506461501121521\n",
      "Step 71: j = 0.5059494972229004\n",
      "Step 72: j = 0.5054553747177124\n",
      "Step 73: j = 0.5049768090248108\n",
      "Step 74: j = 0.5045146346092224\n",
      "Step 75: j = 0.5040682554244995\n",
      "Step 76: j = 0.5036361217498779\n",
      "Step 77: j = 0.5032184720039368\n",
      "Step 78: j = 0.5028150677680969\n",
      "Step 79: j = 0.5024237036705017\n",
      "Step 80: j = 0.5020467638969421\n",
      "Step 81: j = 0.5016814470291138\n",
      "Step 82: j = 0.5013286471366882\n",
      "Step 83: j = 0.5009879469871521\n",
      "Step 84: j = 0.5006575584411621\n",
      "Step 85: j = 0.500339150428772\n",
      "Step 86: j = 0.5000311732292175\n",
      "Step 87: j = 0.49973320960998535\n",
      "Step 88: j = 0.49944421648979187\n",
      "Step 89: j = 0.4991655647754669\n",
      "Step 90: j = 0.4988962709903717\n",
      "Step 91: j = 0.4986352324485779\n",
      "Step 92: j = 0.4983829855918884\n",
      "Step 93: j = 0.4981391727924347\n",
      "Step 94: j = 0.49790289998054504\n",
      "Step 95: j = 0.49767500162124634\n",
      "Step 96: j = 0.49745461344718933\n",
      "Step 97: j = 0.49724099040031433\n",
      "Step 98: j = 0.49703407287597656\n",
      "Step 99: j = 0.4968349039554596\n",
      "j = 0.4966414272785187\n",
      "th = \n",
      "[[ 2.0506616 ]\n",
      " [ 0.91679144]\n",
      " [ 0.14602467]\n",
      " [-0.30913824]\n",
      " [ 0.3107723 ]\n",
      " [ 0.04438523]\n",
      " [-0.10693537]\n",
      " [-0.7071512 ]\n",
      " [-0.6855947 ]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "n_steps = 100\n",
    "display_interval = 1\n",
    "for i_step in range(n_steps): \n",
    "  with tf.GradientTape() as tape:\n",
    "    j = j_func(th)\n",
    "\n",
    "  dj_dth = tape.gradient(j, th)\n",
    "\n",
    "  optimizer.apply_gradients(zip([dj_dth], [th])) # update using gradient\n",
    "\n",
    "  # Print the change of the objective function J\n",
    "  # not printing out the derivative and th as that will be a lot of output, but feel free to add back if you are interest in their movements.\n",
    "  if i_step % display_interval == 0:\n",
    "    print(f'Step {i_step}: j = {j.numpy()}')\n",
    "\n",
    "j = j_func(th)\n",
    "print(f'j = {j.numpy()}\\nth = \\n{th.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7aHAb1vo5Rj"
   },
   "source": [
    "### 2.4.4 Make predictions for the target values using our parameters\n",
    "By considering our problem mathematicall, as matrices, we can make our model predictions by multiplying our new data with the parameter vector we obtained above from our training - $h_{\\boldsymbol{\\theta}} (\\boldsymbol{X_{new}}) = \\boldsymbol{\\hat{y}} =  \\boldsymbol{X_{new}} \\boldsymbol{\\theta}$\n",
    "\n",
    "\n",
    "Note: Make sure that any preprocessing done on `X_train` are also done to `X_test` (in this case standardisation and appending a column of 1's). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcmeTshG9jD3",
    "outputId": "6a4900bf-2932-453d-9da9-08b013708551"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.2005138 ,  2.759748  ,  1.9481404 ,  1.1246605 ,  2.8896115 ,\n",
       "        2.0267205 ,  2.8874698 ,  3.0180454 ,  3.026866  ,  2.2844834 ,\n",
       "        2.2791395 ,  1.0445206 ,  0.8356216 ,  1.4556385 ,  2.1212845 ,\n",
       "        2.7725534 ,  3.4174798 ,  1.9949108 ,  1.8786888 ,  2.4553254 ,\n",
       "        1.5258145 ,  0.9630461 ,  2.854268  ,  2.4214535 ,  1.7302825 ,\n",
       "        1.6167221 ,  2.0193577 ,  2.1621737 ,  1.7206345 ,  0.9924121 ,\n",
       "        3.3999183 ,  2.9302325 ,  2.5468786 ,  1.910693  ,  0.6601673 ,\n",
       "        1.6152725 ,  1.8591053 ,  1.5327145 ,  2.3329792 ,  1.9885747 ,\n",
       "        1.9455469 ,  1.3954854 ,  1.2034775 ,  2.3640885 ,  2.211987  ,\n",
       "        0.45574808,  1.7423255 ,  1.8068651 ,  2.3053749 ,  2.5864778 ,\n",
       "        3.324499  ,  1.2878953 ,  2.6587996 ,  1.4509503 ,  1.4719124 ,\n",
       "        1.6984377 ,  0.9754816 ,  2.0319312 ,  2.1218524 ,  3.0654101 ,\n",
       "        1.7075711 ,  1.6990371 ,  1.1997311 ,  2.4067833 ,  1.4344954 ,\n",
       "        1.5121778 ,  1.2698958 ,  3.3937294 ,  2.3104496 ,  1.2468905 ,\n",
       "        4.361031  ,  1.6776783 ,  1.5872993 ,  0.82524955,  2.4656754 ,\n",
       "        0.86707175,  2.4210067 ,  1.8690423 ,  2.3832545 ,  2.0951862 ,\n",
       "        2.7829065 ,  2.3848307 ,  1.121741  ,  2.2399557 ,  2.5569913 ,\n",
       "        2.3455215 ,  0.6706626 ,  1.6347134 ,  1.1958652 ,  2.1827333 ,\n",
       "        2.4888282 ,  2.2149253 ,  1.0368472 ,  2.567415  ,  0.7745228 ,\n",
       "        2.265079  ,  2.495451  ,  2.7048562 ,  0.84745055,  2.9901466 ,\n",
       "        1.8176746 ,  2.238568  ,  1.6253622 ,  2.5452065 ,  2.2504916 ,\n",
       "        2.7214265 ,  1.5705388 ,  1.117742  ,  0.9792251 ,  1.6284924 ,\n",
       "        2.6033971 ,  1.114188  ,  1.9539121 ,  2.8886828 ,  2.015079  ,\n",
       "        2.9306202 ,  1.7182225 ,  2.21441   ,  1.7732804 ,  1.3522782 ,\n",
       "        2.934667  ,  1.469112  ,  1.8396907 ,  2.234778  ,  2.417265  ,\n",
       "        3.4630852 ,  1.4131273 ,  2.8127394 ,  2.3841972 ,  2.2604156 ,\n",
       "        1.5473832 ,  2.2693925 ,  1.3039732 ,  2.4822025 ,  0.5986823 ,\n",
       "        1.6721642 ,  2.759519  ,  2.7822394 ,  1.5602274 ,  0.3842547 ,\n",
       "        0.96793956,  2.1304626 ,  1.6820467 ,  1.9836702 ,  1.8158906 ,\n",
       "        1.3688142 ,  2.5313525 ,  2.4383924 ,  0.10321069,  2.065251  ,\n",
       "        2.8654318 ,  4.315673  ,  0.95858294,  1.1070118 ,  1.463643  ,\n",
       "        3.2635431 ,  2.4884453 ,  1.5040748 ,  0.5247424 ,  2.0733953 ,\n",
       "        2.5128384 ,  1.8167772 ,  3.0876791 ,  0.5432377 ,  2.1826682 ,\n",
       "        2.0872965 ,  2.0075715 ,  1.6932805 ,  3.6606529 ,  2.4451594 ,\n",
       "        2.0954213 ,  0.76608384,  3.2801914 ,  1.5495589 ,  1.7803583 ,\n",
       "        2.5513208 ,  1.8001043 ,  2.138456  ,  3.314596  ,  1.5191451 ,\n",
       "        0.9568634 ,  2.5538008 ,  2.4374006 ,  1.1300411 ,  1.2704914 ,\n",
       "        1.7149165 ,  2.1163445 ,  2.92596   ,  1.7032752 ,  0.90929425,\n",
       "        2.5795612 ,  2.6482205 ,  2.332476  ,  2.6232686 ,  1.2728659 ,\n",
       "        1.980542  ,  1.4503216 ,  1.8134491 ,  3.567393  ,  1.8402641 ,\n",
       "        2.4164743 ,  2.2651274 ,  1.9586185 ,  0.77915704,  1.3451217 ,\n",
       "        7.0546384 ,  1.7007914 ,  2.610242  ,  0.87805307,  3.4563007 ,\n",
       "        3.482294  ,  0.9530426 ,  2.567996  ,  2.427475  ,  2.587619  ,\n",
       "        1.5220795 ,  0.79730177,  1.4955895 ,  1.737669  ,  1.5679388 ,\n",
       "        1.9038868 ,  2.1099079 ,  2.482909  ,  1.7222835 ,  3.118279  ,\n",
       "        1.1966527 ,  1.3059827 ,  2.6358137 ,  2.5277014 ,  0.65168417,\n",
       "        2.454123  ,  0.6666049 ,  0.72122777,  1.4912509 ,  1.6965499 ,\n",
       "        0.9056964 ,  3.8877473 ,  2.5127997 ,  2.033261  ,  2.2319283 ,\n",
       "        2.016972  ,  2.685848  ,  2.5252585 ,  1.5339084 ,  1.9898622 ,\n",
       "        1.6004881 ,  2.778598  ,  1.7029215 ,  1.5782578 ,  1.5188348 ,\n",
       "        3.2611187 ,  1.6014478 ,  2.2358549 ,  1.5418326 ,  1.8603879 ,\n",
       "        1.4171795 ,  1.7865281 ,  2.160181  ,  2.6240387 ,  2.49149   ,\n",
       "        1.9786532 ,  2.412935  ,  2.0928657 ,  1.824091  ,  1.1642698 ,\n",
       "        2.1502    ,  3.9846945 ,  1.0217438 ,  2.303472  ,  2.0037887 ,\n",
       "        2.9911594 ,  2.3060286 ,  1.3102738 ,  3.6880531 ,  2.0451984 ,\n",
       "        1.7695411 ,  3.564357  ,  1.0087702 ,  1.1354934 ,  1.6034896 ,\n",
       "        1.6375881 ,  2.054984  ,  1.1629181 ,  2.0287197 ,  1.5854309 ,\n",
       "        1.8361021 ,  1.52883   ,  3.4757879 ,  2.2367969 ,  2.1120348 ,\n",
       "        2.1858623 ,  2.407234  ,  3.6858308 ,  1.2385567 ,  1.1928165 ,\n",
       "        2.1943874 ,  1.5456815 ,  2.2189884 ,  1.7829214 ,  2.0916696 ,\n",
       "        1.5912585 ,  1.0098159 ,  2.3806458 ,  2.1867123 ,  1.8812517 ,\n",
       "        1.9693637 ,  2.0426452 ,  2.5823467 ,  2.306751  ,  1.5412538 ,\n",
       "        1.7679937 ,  1.9808179 ,  1.9623702 ,  0.7441081 ,  1.3502247 ,\n",
       "        2.696597  ,  4.720501  ,  2.6694136 ,  0.21488672,  2.92141   ,\n",
       "        2.1073017 ,  2.3053246 ,  6.216718  ,  2.02904   ,  1.6253834 ,\n",
       "        1.6692238 ,  1.90645   ,  3.3135343 ,  2.6126556 ,  2.4536068 ,\n",
       "        2.8993144 ,  1.7267811 ,  1.6693035 ,  1.8756865 ,  1.5469329 ,\n",
       "        2.6032226 ,  1.7338305 ,  0.6046524 ,  1.927791  ,  1.7153487 ,\n",
       "        1.8385743 ,  1.5814836 ,  2.1823244 ,  1.9247277 ,  1.1802446 ,\n",
       "        2.0608408 ,  1.0544162 ,  1.8386352 ,  2.761188  ,  2.5102909 ,\n",
       "        1.3123634 ,  2.293476  ,  2.7428746 ,  1.0253634 ,  0.7624145 ,\n",
       "        2.6231258 ,  2.2309644 ,  2.1480613 ,  2.7038484 ,  2.112102  ,\n",
       "        1.8419203 ,  1.7267654 ,  1.9565163 ,  1.8703636 ,  1.695849  ,\n",
       "        2.0298111 ,  2.982144  ,  2.9149106 ,  3.1422324 ,  2.352201  ,\n",
       "        1.5192301 ,  2.0539935 ,  1.6278236 ,  2.3605652 ,  1.6638072 ,\n",
       "        3.289826  ,  2.4258282 ,  2.4901896 ,  3.3584957 ,  2.8924484 ,\n",
       "        1.2938961 ,  1.8714961 ,  2.3617098 ,  1.1607637 ,  2.2034736 ,\n",
       "        2.4210107 ,  2.0474486 ,  1.7697952 ,  2.6452188 ,  0.8430805 ,\n",
       "        3.159777  , -0.07243782,  1.6758095 ,  2.4386625 ,  1.573433  ,\n",
       "        1.208703  ,  2.2999506 ,  1.9597495 ,  2.6451845 ,  1.2065276 ,\n",
       "        0.965898  ,  1.953991  ,  2.7830915 ,  2.0104647 ,  3.0549755 ,\n",
       "        2.1085777 ,  2.5734553 ,  2.338895  ,  1.005041  ,  2.1985383 ,\n",
       "        0.6770613 ,  0.9811075 ,  1.5711575 ], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_stded = scaler.transform(X_test) # Standardise new data\n",
    "X_test_stded_with_one = append_one_to(X_test_stded) # Add a new \n",
    "\n",
    "# Make your predictions\n",
    "y_pred = tf.squeeze(X_test_stded_with_one @ th) # squeeze Makes matrix into a vector\n",
    "\n",
    "y_pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8tqc72_bk0z"
   },
   "source": [
    "Now let's try visualising our results and getting our evaluations metrics to see how well we have fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "VaO1eWEg8c0T",
    "outputId": "3246a599-400e-48b3-e1d1-e97fd2fca81d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error loss: 0.6991\n",
      "R2 score: 0.4881\n"
     ]
    }
   ],
   "source": [
    "# Plot outputs\n",
    "X_test_disp = X_test[:,0]\n",
    "plt.scatter(X_test_disp, y_test,  color='black', label='y_train')\n",
    "plt.scatter(X_test_disp, y_pred, color='blue', label='y_pred')\n",
    "plt.xlabel('MedInc')\n",
    "plt.ylabel('house price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The mean squared error loss\n",
    "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
    "# The R2 score: 1 is perfect prediction\n",
    "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2O5Ao9tb4z8"
   },
   "source": [
    "### 2.4.5 Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPf6LHpcCxO"
   },
   "source": [
    "Try implementing the SGD method to fit a model for the diabetes dataset you used last week - you can access it using `sklearn.datasets.load_diabetes()`.\n",
    "Once you have a model, try and see how various things impact the model accuracy and the optimisation process.\n",
    "1.   Compare the performance of the model when using normalisation (MinMax scaling) compared to the Standardisation we have been using.\n",
    "2.   Experiment with the learning rate - what happens if you increase the learning rate to '1' or '0.001'? (you may have to alter the number of steps/display rate to get a good idea of what is happening).\n",
    "3. Try Running the code without any preprocessing (standardisation or normalisation), how well does the optimisation behave? Compare the final parameter vector with the one you obtained with preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hsHRD1nNcBBm",
    "outputId": "42db5542-1bcd-4cf3-b0f5-e427509bcce4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8691293f-2869-4d73-9db1-489555bef171\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8691293f-2869-4d73-9db1-489555bef171')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8691293f-2869-4d73-9db1-489555bef171 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8691293f-2869-4d73-9db1-489555bef171');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = \n",
      "[ 8.34669453e-04  1.80409126e-03 -6.94762087e-04 -6.60729781e-04\n",
      " -5.04837638e-04 -8.83187403e-04 -2.17215828e-04  4.24639839e-04\n",
      "  3.02594295e-05 -1.85912442e-04], \n",
      "var = \n",
      "[0.00234483 0.00227008 0.00238975 0.00221343 0.00221956 0.00221696\n",
      " 0.00222709 0.002309   0.00239931 0.00243463]\n",
      "mean = \n",
      "[-3.96283573e-17  8.86920377e-17  5.59829174e-17 -4.85919143e-17\n",
      "  2.86204802e-17  8.64904623e-18 -2.26447756e-17 -1.10078770e-17\n",
      "  2.32737971e-17  2.07577109e-17], \n",
      "var = \n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Step 0: j = 29115.146484375\n",
      "Step 1: j = 18424.81640625\n",
      "Step 2: j = 12745.76171875\n",
      "Step 3: j = 9150.4287109375\n",
      "Step 4: j = 6852.1123046875\n",
      "Step 5: j = 5381.2412109375\n",
      "Step 6: j = 4439.27197265625\n",
      "Step 7: j = 3835.6103515625\n",
      "Step 8: j = 3448.499267578125\n",
      "Step 9: j = 3200.07177734375\n",
      "Step 10: j = 3040.513671875\n",
      "Step 11: j = 2937.930419921875\n",
      "Step 12: j = 2871.904541015625\n",
      "Step 13: j = 2829.343505859375\n",
      "Step 14: j = 2801.85693359375\n",
      "Step 15: j = 2784.068115234375\n",
      "Step 16: j = 2772.518310546875\n",
      "Step 17: j = 2764.990478515625\n",
      "Step 18: j = 2760.05859375\n",
      "Step 19: j = 2756.80615234375\n",
      "Step 20: j = 2754.645263671875\n",
      "Step 21: j = 2753.190673828125\n",
      "Step 22: j = 2752.197021484375\n",
      "Step 23: j = 2751.506591796875\n",
      "Step 24: j = 2751.017578125\n",
      "Step 25: j = 2750.662353515625\n",
      "Step 26: j = 2750.391845703125\n",
      "Step 27: j = 2750.181396484375\n",
      "Step 28: j = 2750.01513671875\n",
      "Step 29: j = 2749.874267578125\n",
      "Step 30: j = 2749.753662109375\n",
      "Step 31: j = 2749.64892578125\n",
      "Step 32: j = 2749.5517578125\n",
      "Step 33: j = 2749.46484375\n",
      "Step 34: j = 2749.381103515625\n",
      "Step 35: j = 2749.302490234375\n",
      "Step 36: j = 2749.226806640625\n",
      "Step 37: j = 2749.156005859375\n",
      "Step 38: j = 2749.08642578125\n",
      "Step 39: j = 2749.017822265625\n",
      "Step 40: j = 2748.9521484375\n",
      "Step 41: j = 2748.88623046875\n",
      "Step 42: j = 2748.8232421875\n",
      "Step 43: j = 2748.75927734375\n",
      "Step 44: j = 2748.69775390625\n",
      "Step 45: j = 2748.636962890625\n",
      "Step 46: j = 2748.576416015625\n",
      "Step 47: j = 2748.515625\n",
      "Step 48: j = 2748.4580078125\n",
      "Step 49: j = 2748.399658203125\n",
      "Step 50: j = 2748.3388671875\n",
      "Step 51: j = 2748.28173828125\n",
      "Step 52: j = 2748.22509765625\n",
      "Step 53: j = 2748.167724609375\n",
      "Step 54: j = 2748.112060546875\n",
      "Step 55: j = 2748.05712890625\n",
      "Step 56: j = 2748.00146484375\n",
      "Step 57: j = 2747.946533203125\n",
      "Step 58: j = 2747.89208984375\n",
      "Step 59: j = 2747.837890625\n",
      "Step 60: j = 2747.78369140625\n",
      "Step 61: j = 2747.728515625\n",
      "Step 62: j = 2747.67626953125\n",
      "Step 63: j = 2747.623779296875\n",
      "Step 64: j = 2747.5703125\n",
      "Step 65: j = 2747.52001953125\n",
      "Step 66: j = 2747.46630859375\n",
      "Step 67: j = 2747.41650390625\n",
      "Step 68: j = 2747.364501953125\n",
      "Step 69: j = 2747.31396484375\n",
      "Step 70: j = 2747.263427734375\n",
      "Step 71: j = 2747.212646484375\n",
      "Step 72: j = 2747.162841796875\n",
      "Step 73: j = 2747.113037109375\n",
      "Step 74: j = 2747.06494140625\n",
      "Step 75: j = 2747.014404296875\n",
      "Step 76: j = 2746.96630859375\n",
      "Step 77: j = 2746.916015625\n",
      "Step 78: j = 2746.868408203125\n",
      "Step 79: j = 2746.820068359375\n",
      "Step 80: j = 2746.77197265625\n",
      "Step 81: j = 2746.723388671875\n",
      "Step 82: j = 2746.677001953125\n",
      "Step 83: j = 2746.631103515625\n",
      "Step 84: j = 2746.583251953125\n",
      "Step 85: j = 2746.535888671875\n",
      "Step 86: j = 2746.490478515625\n",
      "Step 87: j = 2746.4443359375\n",
      "Step 88: j = 2746.3984375\n",
      "Step 89: j = 2746.351806640625\n",
      "Step 90: j = 2746.30908203125\n",
      "Step 91: j = 2746.262451171875\n",
      "Step 92: j = 2746.21728515625\n",
      "Step 93: j = 2746.173095703125\n",
      "Step 94: j = 2746.129150390625\n",
      "Step 95: j = 2746.083251953125\n",
      "Step 96: j = 2746.03955078125\n",
      "Step 97: j = 2745.995361328125\n",
      "Step 98: j = 2745.951416015625\n",
      "Step 99: j = 2745.90869140625\n",
      "j = 2745.864990234375\n",
      "th = \n",
      "[[151.6062   ]\n",
      " [ -1.5240502]\n",
      " [-11.469267 ]\n",
      " [ 27.967691 ]\n",
      " [ 14.1264515]\n",
      " [ -6.9909134]\n",
      " [ -3.9947393]\n",
      " [ -9.759684 ]\n",
      " [  4.921471 ]\n",
      " [ 26.193724 ]\n",
      " [  2.258567 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error loss: 3461.2503\n",
      "R2 score: 0.3250\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "X_pd, y_pd = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "display(X_pd)\n",
    "\n",
    "X = X_pd.to_numpy()\n",
    "y = y_pd.to_numpy() \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # calculate the mean and variance for each feature and store to attributes\n",
    "print(f'mean = \\n{scaler.mean_}, \\nvar = \\n{scaler.var_}') # print the calculated mean and variance for each attribute\n",
    "X_train_stded = scaler.transform(X_train) # standardize X_train\n",
    "print(f'mean = \\n{np.mean(X_train_stded, axis=0)}, \\nvar = \\n{np.var(X_train_stded, axis=0)}') # verify that X_train_stded has mean 0 (mean isn't quite 0 due to numerical error, but is a miniscule value) and variance 1.\n",
    "\n",
    "# Create 1 appending function\n",
    "def append_one_to(X_without_one):\n",
    "  X_with_one = np.pad(X_without_one, ((0, 0), (1, 0)), constant_values=1)\n",
    "  return X_with_one\n",
    "\n",
    "# Use this function to update our training data\n",
    "X_train_stded_with_one = append_one_to(X_train_stded)\n",
    "\n",
    "X = tf.constant(X_train_stded_with_one, dtype=np.float32) # Note, we can put a matrix in as a vector\n",
    "y = tf.constant(y_train[:, np.newaxis], dtype=np.float32) # We use y as a column vector, that is, m x 1 2D array, not an 1D array. For this reason, we input y_train[:, np.newaxis]\n",
    "m, n = X_train_stded_with_one.shape # We use the shape function to obtain our number of rows - `m` and columns - 'n'\n",
    "th = tf.Variable(tf.zeros([n, 1], dtype=np.float32)) # Create a vector of zeroes for our initial guess\n",
    "\n",
    "def j_func(th):\n",
    "  j = (1. / m ) * (tf.transpose(X @ th - y) @ (X @ th - y))\n",
    "  j = tf.squeeze(j) # the `squeeze` function takes our 1x1 matrix and turns it into a scalar value.\n",
    "  return j\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "n_steps = 100\n",
    "display_interval = 1\n",
    "for i_step in range(n_steps): \n",
    "  with tf.GradientTape() as tape:\n",
    "    j = j_func(th)\n",
    "\n",
    "  dj_dth = tape.gradient(j, th)\n",
    "\n",
    "  optimizer.apply_gradients(zip([dj_dth], [th])) # update using gradient\n",
    "\n",
    "  # Print the change of the objective function J\n",
    "  # not printing out the derivative and th as that will be a lot of output, but feel free to add back if you are interest in their movements.\n",
    "  if i_step % display_interval == 0:\n",
    "    print(f'Step {i_step}: j = {j.numpy()}')\n",
    "\n",
    "j = j_func(th)\n",
    "print(f'j = {j.numpy()}\\nth = \\n{th.numpy()}')\n",
    "\n",
    "X_test_stded = scaler.transform(X_test) # Standardise new data\n",
    "X_test_stded_with_one = append_one_to(X_test_stded) # Add a new \n",
    "\n",
    "# Make your predictions\n",
    "y_pred = tf.squeeze(X_test_stded_with_one @ th) # squeeze Makes matrix into a vector\n",
    "\n",
    "y_pred.numpy()\n",
    "\n",
    "# Plot outputs\n",
    "X_test_disp = X_test[:,0] # note we are using unscaled values for our plot. You can use scaled valus, but make sure you don't get confused.\n",
    "plt.scatter(X_test_disp, y_test,  color='black', label='y_train')\n",
    "plt.scatter(X_test_disp, y_pred, color='blue', label='y_pred')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('target')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The mean squared error loss\n",
    "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
    "# The R2 score: 1 is perfect prediction\n",
    "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SLKHEc3gjquo"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
